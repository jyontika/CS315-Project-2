{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using OpenAI Whisper**\n",
    "#### Transcribing TikTok videos\n",
    "\n",
    "##### *Author: @Jyontika Kapoor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/hzlwrw5d571cgm5snsf618b00000gn/T/ipykernel_2772/3692569134.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from docx import Document\n",
    "import os\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import json \n",
    "import librosa\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      7288055765938670894\n",
      "1      7302611720214957358\n",
      "2      7301777464840277294\n",
      "3      7289551496524467502\n",
      "4      7289467129089461550\n",
      "              ...         \n",
      "470    7305077915966852358\n",
      "471    7305541674766241054\n",
      "472    7290970440023969057\n",
      "473    7295843462497226015\n",
      "474    7303352147972721962\n",
      "Name: video_id, Length: 475, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Merging Audrey and Tayae's videos\n",
    "\n",
    "poster = pd.read_csv(\"/Users/jyontika/Documents/GitHub/CS315-Project-2/analysis/news_accs/news_by_poster.csv\")\n",
    "\n",
    "hashtag = pd.read_csv(\"/Users/jyontika/Documents/GitHub/CS315-Project-2/analysis/hashtag_initial/news_by_hashtag.csv\")\n",
    "\n",
    "nyt = pd.read_csv(\"/Users/jyontika/Documents/GitHub/CS315-Project-2/analysis/top_cosine_similarities.csv\")\n",
    "\n",
    "\n",
    "# Merge the DataFrames and keep only the 'video_id' column\n",
    "merged_df = pd.concat([poster['video_id'], hashtag['video_id'], nyt['video_id']], ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# shuffle the indices randomly\n",
    "merged_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>author_username</th>\n",
       "      <th>video_description</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>suggested_words</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6988185257426128133</td>\n",
       "      <td>gma</td>\n",
       "      <td>Morgan Wallen is speaking out to address using...</td>\n",
       "      <td>['news', 'morganwallen']</td>\n",
       "      <td>morgan wallen racist video, morgan wallen, mor...</td>\n",
       "      <td>Sec2Gr3_77777.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7283179469001461035</td>\n",
       "      <td>nbcnews</td>\n",
       "      <td>After wrapping \"#Scandal,\" #KerryWashington's ...</td>\n",
       "      <td>['scandal,\"', \"kerrywashington's\", 'dna']</td>\n",
       "      <td>shera kerry washington, kerry washington somal...</td>\n",
       "      <td>Sec2Gr3_77217.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6948100100992322821</td>\n",
       "      <td>cbsnews</td>\n",
       "      <td>A baby kangaroo is rescued from its motherâ€™s p...</td>\n",
       "      <td>['news', 'australia']</td>\n",
       "      <td>kangaroo pouch, Kangaroo, baby kangaroo, Anima...</td>\n",
       "      <td>Sec2Gr3_77217.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7307673039922105643</td>\n",
       "      <td>wired</td>\n",
       "      <td>Elmo is here to set the record straight, once ...</td>\n",
       "      <td>['elmo.', 'sesamestreet', 'newyorker', 'newyor...</td>\n",
       "      <td>elmo, elmo funniest moments, elmo balsamicvine...</td>\n",
       "      <td>Sec2Gr3_77217.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7306609553305652510</td>\n",
       "      <td>nbcnews</td>\n",
       "      <td>Former President #JimmyCarter and former first...</td>\n",
       "      <td>['jimmycarter', 'rosalynncarter']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sec2Gr3_77217.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7288868567003614510</td>\n",
       "      <td>todayshow</td>\n",
       "      <td>#taylorswift has arrived at the #tserastourmov...</td>\n",
       "      <td>['taylorswift', 'tserastourmovie']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sec2Gr3_74721.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7267091322559843616</td>\n",
       "      <td>bbcnews</td>\n",
       "      <td>One Hawaii resident says some tourists are car...</td>\n",
       "      <td>['lahaina', 'maui', 'hawaii', 'hawaiifire', 'w...</td>\n",
       "      <td>hawaii tourists, maui tourists, hawaii, Lahain...</td>\n",
       "      <td>Sec2Gr3_74721.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7287837532161625377</td>\n",
       "      <td>middleeasteye</td>\n",
       "      <td>Palestinian ambassador to the UK blasts the BB...</td>\n",
       "      <td>['bbc', 'israel', 'palestinian', 'ambassador',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sec2Gr3_74721.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7285777286316510470</td>\n",
       "      <td>ctvnews</td>\n",
       "      <td>If youâ€™re stuck in traffic on Hwy. 400 Tuesday...</td>\n",
       "      <td>['breakingnews', 'truck', 'highway', 'ontario'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sec2Gr3_74721.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>7284629865473625387</td>\n",
       "      <td>espn</td>\n",
       "      <td>What a tackle though ðŸ˜‚ (via @Matt Jensen) #bas...</td>\n",
       "      <td>['baseball', 'football', 'tackle', 'funny']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sec2Gr3_74721.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               video_id author_username  \\\n",
       "0   6988185257426128133             gma   \n",
       "1   7283179469001461035         nbcnews   \n",
       "2   6948100100992322821         cbsnews   \n",
       "3   7307673039922105643           wired   \n",
       "4   7306609553305652510         nbcnews   \n",
       "..                  ...             ...   \n",
       "81  7288868567003614510       todayshow   \n",
       "82  7267091322559843616         bbcnews   \n",
       "83  7287837532161625377   middleeasteye   \n",
       "84  7285777286316510470         ctvnews   \n",
       "85  7284629865473625387            espn   \n",
       "\n",
       "                                    video_description  \\\n",
       "0   Morgan Wallen is speaking out to address using...   \n",
       "1   After wrapping \"#Scandal,\" #KerryWashington's ...   \n",
       "2   A baby kangaroo is rescued from its motherâ€™s p...   \n",
       "3   Elmo is here to set the record straight, once ...   \n",
       "4   Former President #JimmyCarter and former first...   \n",
       "..                                                ...   \n",
       "81  #taylorswift has arrived at the #tserastourmov...   \n",
       "82  One Hawaii resident says some tourists are car...   \n",
       "83  Palestinian ambassador to the UK blasts the BB...   \n",
       "84  If youâ€™re stuck in traffic on Hwy. 400 Tuesday...   \n",
       "85  What a tackle though ðŸ˜‚ (via @Matt Jensen) #bas...   \n",
       "\n",
       "                                             hashtags  \\\n",
       "0                            ['news', 'morganwallen']   \n",
       "1           ['scandal,\"', \"kerrywashington's\", 'dna']   \n",
       "2                               ['news', 'australia']   \n",
       "3   ['elmo.', 'sesamestreet', 'newyorker', 'newyor...   \n",
       "4                   ['jimmycarter', 'rosalynncarter']   \n",
       "..                                                ...   \n",
       "81                 ['taylorswift', 'tserastourmovie']   \n",
       "82  ['lahaina', 'maui', 'hawaii', 'hawaiifire', 'w...   \n",
       "83  ['bbc', 'israel', 'palestinian', 'ambassador',...   \n",
       "84  ['breakingnews', 'truck', 'highway', 'ontario'...   \n",
       "85        ['baseball', 'football', 'tackle', 'funny']   \n",
       "\n",
       "                                      suggested_words          file_name  \n",
       "0   morgan wallen racist video, morgan wallen, mor...  Sec2Gr3_77777.csv  \n",
       "1   shera kerry washington, kerry washington somal...  Sec2Gr3_77217.csv  \n",
       "2   kangaroo pouch, Kangaroo, baby kangaroo, Anima...  Sec2Gr3_77217.csv  \n",
       "3   elmo, elmo funniest moments, elmo balsamicvine...  Sec2Gr3_77217.csv  \n",
       "4                                                 NaN  Sec2Gr3_77217.csv  \n",
       "..                                                ...                ...  \n",
       "81                                                NaN  Sec2Gr3_74721.csv  \n",
       "82  hawaii tourists, maui tourists, hawaii, Lahain...  Sec2Gr3_74721.csv  \n",
       "83                                                NaN  Sec2Gr3_74721.csv  \n",
       "84                                                NaN  Sec2Gr3_74721.csv  \n",
       "85                                                NaN  Sec2Gr3_74721.csv  \n",
       "\n",
       "[86 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Link\n",
      "0    https://www.tiktokv.com/share/video/7288055765...\n",
      "1    https://www.tiktokv.com/share/video/7302611720...\n",
      "2    https://www.tiktokv.com/share/video/7301777464...\n",
      "3    https://www.tiktokv.com/share/video/7289551496...\n",
      "4    https://www.tiktokv.com/share/video/7289467129...\n",
      "..                                                 ...\n",
      "470  https://www.tiktokv.com/share/video/7305077915...\n",
      "471  https://www.tiktokv.com/share/video/7305541674...\n",
      "472  https://www.tiktokv.com/share/video/7290970440...\n",
      "473  https://www.tiktokv.com/share/video/7295843462...\n",
      "474  https://www.tiktokv.com/share/video/7303352147...\n",
      "\n",
      "[475 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "##Change the video_ids to URLs\n",
    "\n",
    "urls = [\"https://www.tiktokv.com/share/video/\" + str(video_id) + \"/\" for video_id in merged_df]\n",
    "\n",
    "# convert  list of URLs to a DataFrame\n",
    "urls_df = pd.DataFrame(urls, columns=['Link'])\n",
    "\n",
    "print(urls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Randomly choose 300 and turn to dataframe\n",
    "sampled_urls_df = urls_df.sample(n=300, random_state=42)\n",
    "\n",
    "\n",
    "json_data = sampled_urls_df.to_dict(orient='records')\n",
    "\n",
    "# Export the JSON data to a file\n",
    "with open(\"sampled_tiktok_urls.json\", \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296, 20)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_vids = pd.read_csv(\"videos-downloaded.csv\")\n",
    "downloaded_vids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      7290493831408045330\n",
       "1      7295925652992363822\n",
       "2      7175869072263990533\n",
       "3      7304461225448967467\n",
       "4      7298442585964662046\n",
       "              ...         \n",
       "291    7300191041280412933\n",
       "292    7293283723934453038\n",
       "293    7297044924862647595\n",
       "294    7287549870171245854\n",
       "295    7291460094032645418\n",
       "Name: video_id, Length: 296, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = downloaded_vids['video_id']\n",
    "video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI(api_key=XYZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the WhisperProcessor and WhisperForConditionalGeneration models\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# set forced_decoder_ids to None for unforced context tokens\n",
    "model.config.forced_decoder_ids = None  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/hzlwrw5d571cgm5snsf618b00000gn/T/ipykernel_32177/824772569.py:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "/var/folders/94/hzlwrw5d571cgm5snsf618b00000gn/T/ipykernel_32177/824772569.py:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio for video ID 7298062755058240801 not found.\n",
      "Audio for video ID 7289462001636576545 not found.\n",
      "Audio for video ID 7305077915966852358 not found.\n",
      "Audio for video ID 7287429952792841504 not found.\n",
      "Audio for video ID 7289223591458131231 not found.\n",
      "Audio for video ID 7300367621764091182 not found.\n",
      "Audio for video ID 7292143951392492846 not found.\n",
      "Audio for video ID 7304509572847521070 not found.\n",
      "Audio for video ID 7289551496524467502 not found.\n",
      "Audio for video ID 7267165450637790482 not found.\n",
      "Audio for video ID 7290970440023969057 not found.\n",
      "Audio for video ID 7294334382150651168 not found.\n",
      "Audio for video ID 7291766284549786926 not found.\n",
      "Audio for video ID 7288487297316572421 not found.\n",
      "Audio for video ID 7293537949860367658 not found.\n",
      "Audio for video ID 7290224386152484101 not found.\n",
      "Audio for video ID 7287680374539423018 not found.\n",
      "Audio for video ID 7288531043681062149 not found.\n",
      "Audio for video ID 7290565797792222510 not found.\n",
      "Audio for video ID 7290322507800890670 not found.\n",
      "Audio for video ID 7304895446378433834 not found.\n",
      "Audio for video ID 7302876922126650630 not found.\n",
      "Audio for video ID 7291760333746736416 not found.\n",
      "Audio for video ID 7296562916235431173 not found.\n",
      "Audio for video ID 7305949069636013343 not found.\n",
      "Audio for video ID 7308783780297280800 not found.\n",
      "Audio for video ID 7291341835052535086 not found.\n"
     ]
    }
   ],
   "source": [
    "output_path = \"txt-transcripts/\"\n",
    "\n",
    "# define sampling rate\n",
    "sampling_rate = 16000  \n",
    "\n",
    "for video_id in video_ids:\n",
    "    audio_path = f\"videos/share_video_{video_id}_.mp4\"\n",
    "\n",
    "\n",
    "    if os.path.exists(audio_path):\n",
    "\n",
    "        # load the audio file using librosa\n",
    "        audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "\n",
    "        # process the audio using WhisperProcessor\n",
    "        input_features = processor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "        # generate token ids for transcription\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "        # decode token ids to text\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        # write transcription to a text file named after the video ID\n",
    "        with open(os.path.join(output_path, f\"{video_id}.txt\"), \"w\", encoding=\"utf-8\") as txt:\n",
    "            txt.write(transcription)\n",
    "\n",
    "    else:\n",
    "        # If audio file not found\n",
    "        print(f\"Audio for video ID {video_id} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
