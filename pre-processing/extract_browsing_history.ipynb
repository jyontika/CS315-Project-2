{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Video History from json\n",
    "\n",
    "This notebook is used to extract the Video Browsing History from a user_data.json file downloaded from TikTok.\n",
    "\n",
    "Author: Audrey Yip & Jyontika Kapoor\n",
    "\n",
    "Date: 03-04-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#team members put path to raw data here \n",
    "filename = '/Users/jyontika/Desktop/user_data_jyontika.json' \n",
    "\n",
    "with open(filename, 'r') as myFile:    \n",
    "  data = json.load(myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Videos: 1\n"
     ]
    }
   ],
   "source": [
    "video_URLs = data['Activity']['Video Browsing History']\n",
    "\n",
    "print(\"Number of Videos:\", len(video_URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Browsing Data has been dumped into url-json-raw/Sec2Gr3_31004.json\n"
     ]
    }
   ],
   "source": [
    "# create anonymized json file, per Eni's instructions\n",
    "random_number = str(random.randint(10000, 99999))\n",
    "path_raw = \"url-json-raw/\"\n",
    "filename = \"Sec2Gr3_\" + random_number + \".json\"\n",
    "outfile_path_raw =  os.path.join(path_raw, filename) \n",
    "\n",
    "with open(outfile_path_raw, 'w') as outfile:\n",
    "    json.dump(video_URLs, outfile)\n",
    "\n",
    "print(\"Video Browsing Data has been dumped into\", outfile_path_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dates related to world news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to filter dates\n",
    "\n",
    "def filter_dates(start_date, end_date, data):\n",
    "    filtered_data = []\n",
    "    for entry in data['VideoList']:\n",
    "        date_str = entry['Date']\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if start_date <= date_obj <= end_date:\n",
    "            filtered_data.append(entry)\n",
    "    return {\"VideoList\": filtered_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oct 7 and 2 months after\n",
    "start_date = datetime(2023, 10, 7)\n",
    "end_date = datetime(2023, 12, 7)\n",
    "\n",
    "with open(outfile_path_raw, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Filter the data based on the date range\n",
    "filtered_data = filter_dates(start_date, end_date, json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to url-json-oct7/Sec2Gr3_31004.json\n"
     ]
    }
   ],
   "source": [
    "# save the filtered data to a new JSON file\n",
    "filtered_path = \"url-json-oct7/\"\n",
    "filename = \"Sec2Gr3_\" + random_number + \".json\"\n",
    "outfile_path_filtered =  os.path.join(filtered_path, filename) \n",
    "\n",
    "with open(outfile_path_filtered, 'w') as file:\n",
    "    json.dump(filtered_data['VideoList'], file, indent=2)\n",
    "\n",
    "print(f\"Filtered data saved to {outfile_path_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parking lot for code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert video URLs to a csv -- ONLY with link\n",
    "\n",
    "# video_urls = [entry['Link'] for entry in video_URLs['VideoList']]\n",
    "\n",
    "# path = \"url-csv/\"\n",
    "# filename  = 'video_urls_' + random_number + '.csv' \n",
    "# csv_file_path = os.path.join(path, filename) \n",
    "\n",
    "# # write the URLs into a csv\n",
    "# with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "#     csv_writer = csv.writer(csv_file)\n",
    "#     csv_writer.writerow(['Video URL'])  # Write header\n",
    "\n",
    "#     for url in video_urls:\n",
    "#         csv_writer.writerow([url])\n",
    "\n",
    "# print(\"Video URLs have been dumped into\", csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this chunk of code converted video URLs to CSV with dates\n",
    "\n",
    "# video_data = video_URLs['VideoList']\n",
    "# video_urls = [entry['Link'] for entry in video_data]\n",
    "# video_dates = [entry['Date'] for entry in video_data]\n",
    "\n",
    "# # Combine links and dates into pairs\n",
    "# video_info = zip(video_urls, video_dates)\n",
    "\n",
    "# # Specify the CSV file path\n",
    "# path = \"url-csv/\"\n",
    "# filename = f'video_urls_{random_number}.csv'\n",
    "# csv_file_path = os.path.join(path, filename)\n",
    "\n",
    "# # write the URLs and dates into a csv\n",
    "# with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "#     csv_writer = csv.writer(csv_file)\n",
    "#     csv_writer.writerow(['Video URL', 'Date']) \n",
    "    \n",
    "#     for url, date in video_info:\n",
    "#         csv_writer.writerow([url, date])\n",
    "\n",
    "# print(\"Video URLs and Dates have been dumped into\", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract following list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please:\n",
    "1) Run the first two cells of this notebook, and then these cells\n",
    "2) Copy and paste this list into an email and send it to me, with the name f'acc_{random_number}'\n",
    "3) If your account doesn't have a random number (i.e. it doesn't have video browsing history so you didn't run it above, please just name it 'acc_{your_initials}{counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flatdict\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Following: 162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lochan.k',\n",
       " 'r.ohini',\n",
       " '20amuller',\n",
       " 'thefurnituredoctor',\n",
       " 'ryanisreallypolite',\n",
       " 'cnovello13',\n",
       " 'millymillyrockz',\n",
       " 'lilchikiszat',\n",
       " 'karinaaarose',\n",
       " 'misssmaiah',\n",
       " 'alexamichela',\n",
       " 'urvikap97',\n",
       " 'ayanna_moise',\n",
       " 'nasirwynruit',\n",
       " 'jvmes.music',\n",
       " 'peaceluvcourt',\n",
       " 'ruiepooie',\n",
       " 'xo.zen',\n",
       " 'eeshkapeesh999',\n",
       " 'superkeara',\n",
       " 'selenagomez',\n",
       " 'karissalauren',\n",
       " 'phillyfoodies',\n",
       " 'itzelt.reyes',\n",
       " 'shruthisundar01',\n",
       " 'imogenieinabottle',\n",
       " 'ripberniemadoff',\n",
       " 'sslizzle',\n",
       " 'alex.ndratx',\n",
       " 'srimyla',\n",
       " 'alakeyeah',\n",
       " 'sachitanwar6',\n",
       " 'thewellesleynews',\n",
       " 'gshell08',\n",
       " 'oliviaholtzinger10',\n",
       " 'mkennedy28',\n",
       " 'box_of_olives',\n",
       " 'cosyeet',\n",
       " 'sophferrante',\n",
       " 'jassaco_xo',\n",
       " 'zoralikedora',\n",
       " 'planetmargs',\n",
       " 'erictran42',\n",
       " 'bhadminton',\n",
       " 'whiteboyofthemonth69',\n",
       " 'papayasguy',\n",
       " 'kenzie461',\n",
       " 'sophdog03',\n",
       " 'ellie.bk',\n",
       " 'evanpalmblad',\n",
       " 'rosedg18',\n",
       " 'averyhirschofficial',\n",
       " 'nationltreasure',\n",
       " 'liesel.liesel',\n",
       " 'ash.reeeee',\n",
       " 'sarahcatherinne_',\n",
       " 'shirtsbylena',\n",
       " 'igobyadi',\n",
       " 'jamsino',\n",
       " 'krithi.com',\n",
       " 'uhavesy0',\n",
       " 'zockjat',\n",
       " 'yung_maud',\n",
       " 'mooptopia',\n",
       " 'emartin20',\n",
       " 'devsdoodlesco',\n",
       " 'thekaranmenon',\n",
       " 'm_crawfish',\n",
       " 'hedabobeda',\n",
       " 'emgems22',\n",
       " 'carozags',\n",
       " 'ameltzz',\n",
       " 'username01242002',\n",
       " 'blackeyedboba',\n",
       " 'shivanisowmyan',\n",
       " 'rsellll',\n",
       " 'em_levy',\n",
       " 'sialateralligator_',\n",
       " 'wholelotofeverything',\n",
       " 'lenamantler',\n",
       " 'sabribriroiz',\n",
       " 'kate.mcb',\n",
       " 'hannawillans',\n",
       " 'laurennhanderson',\n",
       " 'rayrayroobs77',\n",
       " 'wybieandhiscat',\n",
       " 'badgalriyriy',\n",
       " 'lindsay.fleminglpc',\n",
       " 'carly.delong',\n",
       " 'ashleyradell',\n",
       " 'kariskim5',\n",
       " 'emmydeluca711',\n",
       " 'sophstyle4',\n",
       " 'samanthalevin',\n",
       " 'alex53786',\n",
       " 'ohheymanwhatsup',\n",
       " 'kennydink',\n",
       " 'soccerbeb',\n",
       " 'lilphyll',\n",
       " 'shrekdaddy99',\n",
       " 'ladybimbo_',\n",
       " 'freshparm',\n",
       " 'gustavobitch',\n",
       " 'sophiarco',\n",
       " 'g.pellicci',\n",
       " 'grace.mullen22',\n",
       " 'johannabauerr',\n",
       " '_meeg',\n",
       " 'coolnokap',\n",
       " 'rheamayekar5',\n",
       " 'tyler_thefailure',\n",
       " 'jaxn.grunk',\n",
       " 'lauren.arnold',\n",
       " 'ktsap',\n",
       " 'yungbenjii',\n",
       " 'caelanlafferty',\n",
       " 'corinne.laff',\n",
       " 'ashleytisdale',\n",
       " 'fernrose0',\n",
       " 'leswayy',\n",
       " 'elliematata',\n",
       " 'hals.lubes',\n",
       " 'jormakingmecrazy',\n",
       " 'therealestbs',\n",
       " 'malgal19',\n",
       " 'laurenelwell465',\n",
       " 'jweiss19',\n",
       " 'stevesgrocery',\n",
       " 'briannebaker_',\n",
       " 'hannagoldberg18',\n",
       " 'catherinedoingherbest',\n",
       " 'santinobambino19o',\n",
       " 'blandoatmeal',\n",
       " 'erindailyyy',\n",
       " 'aschoppe26',\n",
       " 'slim.sadieee',\n",
       " 'ema_nooooooo',\n",
       " 'funny.youngen',\n",
       " 'mannyoramuppet',\n",
       " 'katelovensheimer',\n",
       " 'a.timm728',\n",
       " 'magnoliaprintzmd',\n",
       " 'bananaphone3894',\n",
       " 'victoriaxchung',\n",
       " 'mari80830',\n",
       " 'liladrivert',\n",
       " 'omb27_',\n",
       " 'annaalberti',\n",
       " 'c0wgirlclaire',\n",
       " 'kingconeofficial',\n",
       " 'chloelovensheimer',\n",
       " 'jaclynstaub',\n",
       " 'evanmustin',\n",
       " 'daviddobrik',\n",
       " 'hansonleung6',\n",
       " 'tessklugz_',\n",
       " 'sounwell',\n",
       " 'hannahehav',\n",
       " 'maddyhipp',\n",
       " 'carly.amato',\n",
       " 'hotsauceelbow',\n",
       " 'uwuchristian1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of Following:\", len(data['Activity']['Following List']['Following'])) # print number of followers\n",
    "\n",
    "d =  flatdict.FlatDict(data['Activity']['Following List'], delimiter='.')['Following'] #flattens dict; gives list\n",
    "res = defaultdict(list)\n",
    "{res[key].append(sub[key]) for sub in d for key in sub} # makes one dict from list of dicts\n",
    "\n",
    "follow_list = res['UserName'] # list of user's followers\n",
    "follow_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/jyontika/Desktop/follow_data.csv'\n",
    "\n",
    "# Write the follow_list to the CSV file\n",
    "with open(file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for item in follow_list:\n",
    "        csv_writer.writerow([item])\n",
    "\n",
    "print(\"CSV file has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
